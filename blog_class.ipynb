{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "import urllib\n",
    "from bs4 import BeautifulSoup\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import sent_tokenize\n",
    "import re\n",
    "from datetime import datetime, date, time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "class Blog(object):\n",
    "    # constants in the class\n",
    "    STOPWORDS = stopwords.words('english') + [x.title() for x in stopwords.words('english')] + [x.upper() for x in stopwords.words('english')]\n",
    "    ADS_SRC = \"https://no-cache.hubspot.com/cta/default/3858309/d5cac7dd-b452-4aaa-b9c6-ded225e29405.png\"\n",
    "    \n",
    "    def __init__(self, url):\n",
    "        self.url = url\n",
    "    \n",
    "    def parse(self):\n",
    "        # parses the html url into a beautiful soup object\n",
    "        soup = BeautifulSoup(urllib.urlopen(self.url).read(), 'html.parser')\n",
    "        return soup\n",
    "    \n",
    "    def display_html(self):\n",
    "        # displays the bs object in clean html format\n",
    "        print self.parse().prettify()\n",
    "        \n",
    "    def title(self):\n",
    "        # returns the title of the blog article\n",
    "        return self.parse().find('h1').get_text()\n",
    "    \n",
    "    def sub_title(self):\n",
    "        # displays all the sub-title of the blog article (this is a list)\n",
    "        return [item.get_text() for item in self.parse().find_all('h2')]\n",
    "    \n",
    "    def num_sub_title(self):\n",
    "        # counts the number of subtitles\n",
    "        return len(self.sub_title())\n",
    "    \n",
    "    def author(self):\n",
    "        # returns the author of the blog article\n",
    "        return self.parse().find(\"a\", {\"class\" : \"author\"}).get_text()\n",
    "    \n",
    "    @staticmethod\n",
    "    def date_format(d):\n",
    "        d = d[10:]\n",
    "        d = d[:3] + ' ' + re.sub('[a-zA-Z]', '', d[4:])\n",
    "        return datetime.strptime(d, \"%b %d, %Y\").date()\n",
    "\n",
    "    def post_date(self):\n",
    "        # returns the post date of the blog article\n",
    "        return Blog.date_format(self.parse().find(\"span\", {\"class\" : \"authordate\"}).get_text())\n",
    "    \n",
    "    def break_into_paragraph(self, ads_src = ADS_SRC):\n",
    "        # returns a list where each element is a paragraph of the blog article\n",
    "        all_paragraphs = self.parse().find(\"div\", {\"class\": \"section post-body\"}).find_all('p')\n",
    "        paragraphs = []\n",
    "        for item in all_paragraphs:\n",
    "            if item.find('a') == None:\n",
    "                paragraphs.append(item)\n",
    "            elif item.find('a').find('img') == None:\n",
    "                paragraphs.append(item)\n",
    "            elif item.find('a').find('img')['src'] != ads_src:\n",
    "                paragraphs.append(item)\n",
    "        return [x.get_text() for x in paragraphs]\n",
    "    \n",
    "    def num_paragraph(self):\n",
    "        # returns the number of paragraphs\n",
    "        return len(self.break_into_paragraph())\n",
    "    \n",
    "    def blog_body(self):\n",
    "        # displays the body of the blog as an article\n",
    "        return '\\n'.join(self.break_into_paragraph())\n",
    "\n",
    "    @staticmethod\n",
    "    def count_word_freq(corpus, rm_stop_words = False, stopwords = STOPWORDS):\n",
    "        # returns the word count frequency of a part of the article\n",
    "        # can do word count for any part with function self.count_word_freq(self.part, True)\n",
    "        to_tokenize = corpus\n",
    "        tokens = [t.lower() for t in nltk.word_tokenize(to_tokenize) if re.match('^(?=.*[a-zA-Z]|[0-9])', t)]\n",
    "        if not rm_stop_words:\n",
    "            freq = nltk.FreqDist(tokens)\n",
    "        else:\n",
    "            clean_tokens = tokens[:]\n",
    "            for token in tokens:\n",
    "                if token in stopwords:\n",
    "                    clean_tokens.remove(token)\n",
    "            freq = nltk.FreqDist(clean_tokens)\n",
    "        return freq\n",
    "    \n",
    "    @staticmethod\n",
    "    def word_count(corpus, rm_stop_words = False):\n",
    "        # returns the actual word count of a part of the article\n",
    "        # it would be a list, with first element being total word count, second element being unique word count\n",
    "        return [sum([value for key, value in Blog.count_word_freq(corpus, rm_stop_words).items()]),\n",
    "                len([key for key, value in Blog.count_word_freq(corpus, rm_stop_words).items()])]\n",
    "    \n",
    "    def title_word_freq(self, rm_stop_words = False):\n",
    "        # returns the word count frequency of title of the article\n",
    "        return self.count_word_freq(self.title(), rm_stop_words)\n",
    "    \n",
    "    def body_word_freq(self, rm_stop_words = False):\n",
    "        # returns the word count frequency of body of the article\n",
    "        return self.count_word_freq(self.blog_body(), rm_stop_words)\n",
    "    \n",
    "    def title_word_count(self, rm_stop_words = False):\n",
    "        # returns the word count of a title\n",
    "        return self.word_count(self.title(), rm_stop_words)\n",
    "    \n",
    "    def body_word_count(self, rm_stop_words = False):\n",
    "        # returns the word count of a title\n",
    "        return self.word_count(self.blog_body(), rm_stop_words)\n",
    "    \n",
    "    @staticmethod\n",
    "    def avg_word_count(corpus_list, rm_stop_words = False):\n",
    "        # counts the average number of word in a list of a corpus\n",
    "        # could count any avg word count through function self.avg_word_count(self.sub_title())\n",
    "        # or access subtitle, paragraph average count using the following functions\n",
    "        lens = [Blog.word_count(x, rm_stop_words)[0] for x in corpus_list]\n",
    "        return round(float(sum(lens))/len(lens), 1)\n",
    "        \n",
    "    def avg_word_count_sub_title(self, rm_stop_words = False):\n",
    "        # counts the average number of word in sub titles\n",
    "        return Blog.avg_word_count(self.sub_title(), rm_stop_words)\n",
    "\n",
    "    def avg_word_count_paragraph(self, rm_stop_words = False):\n",
    "        # count the average number of word in each paragraph\n",
    "        return Blog.avg_word_count(self.break_into_paragraph(), rm_stop_words)\n",
    "    \n",
    "    @staticmethod\n",
    "    def break_into_sentence(corpus):\n",
    "        # breaks a corpus into sentences\n",
    "        return(sent_tokenize(corpus))\n",
    "    \n",
    "    @staticmethod\n",
    "    def sentence_count(corpus):\n",
    "        # counts the number of sentences in a corpus\n",
    "        return len(Blog.break_into_sentence(corpus))\n",
    "    \n",
    "    def sentence_count_body(self):\n",
    "        # counts the total number of sentences in the blog article\n",
    "        return Blog.sentence_count(self.blog_body())\n",
    "    \n",
    "    def avg_sentence_count_paragraph(self):\n",
    "        # counts the average number of sentence in each paragraph\n",
    "        lens = [Blog.sentence_count(x) for x in self.break_into_paragraph()]\n",
    "        return round(float(sum(lens))/len(lens), 1)\n",
    "    \n",
    "    def avg_word_count_in_sentence(self, rm_stop_words = False):\n",
    "        # counts the average number of word in each sentence\n",
    "        return Blog.avg_word_count(Blog.break_into_sentence(test.blog_body()), rm_stop_words)\n",
    "    \n",
    "    def num_graphics(self, ads_src = ADS_SRC):\n",
    "        # counts the images and graphics in the blog article\n",
    "        all_img = self.parse().find(\"div\", {\"class\": \"section post-body\"}).find_all('img')\n",
    "        return len([x for x in all_img if x['src'] != ads_src])\n",
    "    \n",
    "    def num_links(self, ads_src = ADS_SRC):\n",
    "        # counts the hyper text links in the blog article\n",
    "        all_links = self.parse().find(\"div\", {\"class\": \"section post-body\"}).find_all('a')\n",
    "        counter = 0\n",
    "        for link in all_links:\n",
    "            if link.find('img') == None:\n",
    "                counter = counter + 1\n",
    "            elif link.find('img')['src'] != ads_src:\n",
    "                counter = counter + 1\n",
    "        return counter\n",
    "    \n",
    "    @staticmethod\n",
    "    def consolidate_dictionary(d):\n",
    "        # returns a new dictionary consodidating the key values into the broader categories that we care about\n",
    "        new_d = {'verb': 0, 'noun': 0, 'adj': 0, 'adv': 0}\n",
    "        for key, value in d.items():\n",
    "            if key in ['VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ']:\n",
    "                new_d['verb'] = new_d['verb'] + d[key]\n",
    "            elif key in ['RB', 'RBR', 'RBS']:\n",
    "                new_d['adv'] = new_d['adv'] + d[key]\n",
    "            elif key in ['JJ', 'JJR', 'JJS']:\n",
    "                new_d['adj'] = new_d['adj'] + d[key]\n",
    "            elif key in ['NN', 'NNP', 'NNPS', 'NNS']:\n",
    "                new_d['noun'] = new_d['noun'] + d[key]\n",
    "            else:\n",
    "                new_d[key] = d[key]\n",
    "\n",
    "        return new_d\n",
    "\n",
    "    @staticmethod\n",
    "    def word_type(corpus, rm_stop_words = False, stopwords = STOPWORDS):\n",
    "        # returns the word type frequency count of the corpus\n",
    "        # check the details of the classification here nltk.help.upenn_tagset()\n",
    "        if not rm_stop_words:\n",
    "            tokens = [t for t in nltk.word_tokenize(corpus) if re.match('^(?=.*[a-zA-Z]|[0-9])', t)]\n",
    "        else:\n",
    "            tokens = [t for t in nltk.word_tokenize(corpus) if (re.match('^(?=.*[a-zA-Z]|[0-9])', t) and t not in stopwords)]\n",
    "        result = nltk.pos_tag(tokens)\n",
    "        word_class_dict = {}\n",
    "        for x in result:\n",
    "            if x[1] not in word_class_dict:\n",
    "                word_class_dict[x[1]] = 1\n",
    "            else:\n",
    "                word_class_dict[x[1]] = word_class_dict[x[1]] + 1\n",
    "        new_d = Blog.consolidate_dictionary(word_class_dict)\n",
    "        return new_d\n",
    "    \n",
    "    @staticmethod\n",
    "    def word_type_percentage(corpus, rm_stop_words = False, stopwords = STOPWORDS):\n",
    "        # returns the word type frequency percentage of the corpus\n",
    "        word_class = Blog.word_type(corpus, rm_stop_words)\n",
    "        total_words = sum([v for k, v in word_class.items()])\n",
    "        new_dict = {k:round(float(v)/total_words, 2) for k, v in word_class.items()}\n",
    "        return new_dict\n",
    "    \n",
    "    def word_type_percentage_title(self, word_type = 'noun', rm_stop_words = False):\n",
    "        # returns the percentage of different types of words in title\n",
    "        # verb', 'noun', 'adj', 'adv', 'wrb' are the types that we care about\n",
    "        # if needs to return the distribution of each word type, use command:\n",
    "        # self.word_type_percentage(self.title())\n",
    "        pct_dict = self.word_type_percentage(self.title(), rm_stop_words)\n",
    "        return pct_dict[word_type]\n",
    "    \n",
    "    def word_type_percentage_body(self, word_type = 'noun', rm_stop_words = False):\n",
    "        # returns the percentage of different types of words in title\n",
    "        # verb', 'noun', 'adj', 'adv', 'wrb' are the types that we care about\n",
    "        # if needs to return the distribution of each word type, use command:\n",
    "        # self.word_type_percentage(self.blog_body())\n",
    "        pct_dict = self.word_type_percentage(self.blog_body(), rm_stop_words)\n",
    "        return pct_dict[word_type]\n",
    "    \n",
    "    def key_words(self, rm_stop_words = True):\n",
    "        # returns the first 5 most frequent nouns in the article:\n",
    "        freq_dict = self.body_word_freq(rm_stop_words)\n",
    "        sorted_freq = sorted([(k, v) for k, v in freq_dict.items()], \n",
    "                             reverse = True, \n",
    "                            key = lambda x: x[1])\n",
    "        return sorted_freq[:10]\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#test with article\n",
    "# article_url = 'https://blog.aurorasolar.com/expert-qa-why-solar-panel-recycling-matters-and-how-it-can-benefit-the-industry'\n",
    "# test = Blog(article_url)\n",
    "# print test.key_words()\n",
    "# test.title()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# test for functions\n",
    "# print test.title()\n",
    "# print test.sub_title()\n",
    "# print test.num_sub_title()\n",
    "# print test.author()\n",
    "# for x in test.break_into_paragraph():\n",
    "#     print x\n",
    "#     print '\\n'\n",
    "# print test.blog_body()\n",
    "# print test.num_paragraph()\n",
    "# print test.title_word_freq(False)\n",
    "# print test.title_word_count(True)\n",
    "# print test.body_word_count(True)\n",
    "# print test.avg_word_count_sub_title()\n",
    "# print test.avg_word_count_paragraph()\n",
    "# print test.break_into_sentence(test.break_into_paragraph()[0])\n",
    "# print test.sentence_count(test.break_into_paragraph()[0])\n",
    "# print test.sentence_count(test.blog_body())\n",
    "# print test.sentence_count_body()\n",
    "# print test.avg_sentence_count_paragraph()\n",
    "# print test.avg_word_count_in_sentence()\n",
    "# print test.num_graphics()\n",
    "# print test.num_links()\n",
    "# print test.post_date()\n",
    "# print test.title()\n",
    "# print test.word_type_percentage(test.title())\n",
    "# print test.word_type_percentage(test.blog_body())\n",
    "# print test.word_type_percentage_title('adj')\n",
    "# print test.word_type_percentage_body()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
